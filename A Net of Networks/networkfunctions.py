# -*- coding: utf-8 -*-
"""NetworkFunctions

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c5bA-MMIMuNn6gGYrJ2yUWIvCmqz0p5M
"""

import random
import math
import numpy as np

def make_network(Ninput, Nhidden, Noutput):
  net = list()
  hidden = [{'weights':[random.random() for i in range(Ninput + 1)]} for i in range(Nhidden)]
  output = [{'weights':[random.random() for i in range(Nhidden + 1)]} for i in range (Noutput)]
  net.append(hidden)
  net.append(output)
  return net

netted = make_network(3,2,2)

def activate (W, inputs): #W -> weights between this neuron and the previous layer
  outputs = W[-1] #takes out bias so it won't be calculated against weights
  for i in range(len(inputs)):
    outputs += W[i] * inputs[i] #calculates weights and inputs and adds it to the output to get node total
  return outputs

def transfer(x):
  x = np.float128(x)
  squashed_x = np.tanh(x) #values were becoming too small so trying this instead
  #squashed_x = (1 - np.exp(-x)) / (1 + np.exp(-x))
  #squashed_x = (1 - math.pow(math.e, -x)) / (1 + math.pow(math.e, -x)) #issues with math.pow
  return squashed_x #answer should always be between 1 and -1

def forward(net, raw):
  inputed = raw
  for i in net:
    out = []
    #print(net)
    for neuron in i:
      rawOut = activate(neuron['weights'], inputed)
      squashedOut = transfer(rawOut)
      neuron['output'] = squashedOut
      out.append(neuron['output'])
    inputed = out
  #return net
  return out

def numLayers(net):
  #return the number of non-input layers so hidden and output (2)
  #if the ouputs are not what you wanted how can you change the weights between the layers to change the values of the nodes of the output
  return len(net)

def getLayer(net, layerNumber):
  #gives you back a layer
  return net[layerNumber]

def numNeurons(layer):
  #layer is a list of dictionaries got from getlayer
  #returns number of nodes in layer
  return len(layer)

def getNeuron(layer, nodeNumber):
  #returns the dictionary associated with the node being called
  return layer[nodeNumber]

def transfer_derivative(x):
    #return x
    return x * (1.0 - x) #so far x should be between 0 and 1 so subtracting from one we then multiply is by node value by its difference from 1
    #ex. the output value is 0.9 -> 0.9 * 0.1 = 0.09
    #ex. 0.3 -> 0.3 * 0.7 = 0.21

def backProp(net, expected):
    for i in reversed(range(numLayers(net))): #working backwards to figure out where NN went wrong
      layeri = getLayer(net,i) #get layer's list of dictionaries
      errors = list()
      if i == (numLayers(net) -1): #for the last layer
        for j in range(numNeurons(layeri)):
          neuron = getNeuron(layeri, j) #get dictionary of neuron
          errors.append(neuron['output'] - expected[j]) #figure out how far off actual value is from expected and add it to list
      else: #for the hidden layer(s) - this will never be called first
        for j in range(numNeurons(layeri)): #goes through neurons
          error = 0.0
          nextLayer = getLayer(net, i+1) #gets the previous layers list of dicrionaries (dont't forget we are working backwards)
          for neuron in nextLayer: #goes through dictionaries in list of previous layer
            error += (neuron['weights'][j] * neuron['delta']) #delta is the error that is held inside the node
            #print("Weights of previous layer", neuron['weights'][j], "multiplied by significance of error", neuron['delta'])
          errors.append(error)

        #print(errors)

      for j in range(numNeurons(layeri)): #goes throught the range of the number of neurons in current layer
        neuron =  getNeuron(layeri, j) #gets the dictionary of neuron of specified layer
        neuron['delta'] = errors[j] * transfer_derivative(neuron['output']) #we multiply that node's error by the weight which is a fraction of
                                                                            #the weight of the nodes value

# Update weights to reduce error
def update(net, inputs, learning_rate):
    for i in range(numLayers(net)): #goes through the range of the length of the number of layers in the network
        if i == 0: #if we are in the first hidden layer
            feed_into_layer_i = inputs[:-1] #gets the inputs
        else:
            prevLayer = getLayer(net, i-1) #gets the previous layer
            feed_into_layer_i = [neuron['output'] for neuron in prevLayer] #gets the outputs of the previous layer so they will be the current inputs

        thisLayer = getLayer(net, i) #gets current layer
        for neuron in thisLayer: #goes through layer
            for j in range(len(feed_into_layer_i)): #goes through all the neuron numbers of the previous layer
                neuron['weights'][j] -= learning_rate * neuron['delta'] * feed_into_layer_i[j] #multiplying error current error of node by previous nodes value * the learning rate which is a tiny number is how much it can adjust by
            neuron['weights'][-1] -= learning_rate * neuron['delta'] #this relates to the bias so here we update the bias - we reduce the bias by a porportion (learning rate) of the error

# Train a network for a fixed number of epochs
# once we have shown the 6 examples 1 epoch has occured
def do_training(net, train_inputs, train_outputs, learning_rate, n_epoch):
    min = 100000
    for epoch in range(n_epoch):
        sum_error = 0
        for i in range(len(train_inputs)): #loop through all the 6 inputs
            one_training_input = train_inputs[i] #we grab a single input value so either cat or dog
            output_desired = train_outputs[i] #desired node value output (cat or dog)
            #print("Output Desired = ", output_desired)
            output_actual = forward(net, one_training_input) #when we feed forward we are getting the actual node value
            #print("Output Actual = ", output_actual)
            sum_error += sum([(output_desired[i] - output_actual[i]) ** 2 for i in range(len(output_desired))]) #we want to add up all the errors we are making
            #in above line we are seeing the difference of desired - actual and then square it so we only have 1 desired output in case below
            #we are summing up the error occuring in each output
            backProp(net, output_desired) #find the errors
            update(net, one_training_input, learning_rate) #update to reflect adjusted error
        if sum_error < min:
          min = sum_error

    return min

